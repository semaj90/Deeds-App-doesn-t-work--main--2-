# ⚠️  IMPORTANT: USER MODEL REQUIREMENTS
# This application does NOT provide, download, or bundle any LLM models.
# For local LLM features, you must provide your own GGUF models.
# See USER_MODEL_REQUIREMENTS.md for detailed instructions.

# ======================
# PYTHON ENVIRONMENT (Completely isolated from user's PyTorch)
# ======================
# Custom Python environment path (will be created automatically)
# This ensures NO interference with your existing Python/PyTorch setup
PYTHON_VENV_PATH=./venv-legal-nlp
PYTHON_EXECUTABLE=python

# Isolated requirements file to use (prevents PyTorch conflicts)
REQUIREMENTS_FILE=requirements-safe.txt

# ======================
# PYTORCH PRESERVATION (Protect user's installation)
# ======================
# This setting ensures we NEVER interfere with your existing PyTorch
PRESERVE_USER_PYTORCH=true

# Set to 'true' to completely skip PyTorch installation in our environment
# (Some AI features will be limited but document processing will still work)
SKIP_PYTORCH_INSTALL=false

# Custom PyTorch index URL - ONLY set if you need specific version
# Leave empty to respect your existing installation
PYTORCH_INDEX_URL=
# Examples (uncomment only if needed):
# PYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu118  # CUDA 11.8
# PYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu121  # CUDA 12.1
# PYTORCH_INDEX_URL=https://download.pytorch.org/whl/cpu    # CPU only

# ======================
# ENVIRONMENT ISOLATION SETTINGS
# ======================
# These ensure our app doesn't touch your global Python environment
PYTHONPATH=
VIRTUAL_ENV_DISABLE_PROMPT=true
PIP_USER=false

# ======================
# CUDA/GPU SETTINGS
# ======================
# These will only affect our isolated environment
CUDA_VISIBLE_DEVICES=0
USE_GPU=auto
TORCH_DEVICE=auto

# Fallback settings if PyTorch detection fails
FALLBACK_TO_CPU=true
ENABLE_GPU_ACCELERATION=auto

# Database Configuration
# For SQLite (development): 
DATABASE_URL="file:./dev.db"
# For PostgreSQL (production):
# DATABASE_URL="postgresql://postgres:your_password_here@localhost:5432/prosecutor_db"

# Authentication & Security (REQUIRED - Generate secure values)
AUTH_SECRET="your_auth_secret_here_minimum_32_characters_for_security"
JWT_SECRET="your_very_secure_jwt_secret_key_here_at_least_32_characters_long_for_security"
SESSION_SECRET="your_secure_session_secret_here_minimum_32_chars_long_for_proper_encryption"
BCRYPT_ROUNDS=12

# OAuth (optional)
GOOGLE_CLIENT_ID="YOUR_GOOGLE_CLIENT_ID"
GOOGLE_CLIENT_SECRET="YOUR_GOOGLE_CLIENT_SECRET"

# Application Settings
NODE_ENV=development
ORIGIN=http://localhost:5173
ENABLE_REGISTRATION=true
DEFAULT_USER_ROLE=prosecutor

# File Upload & Storage
UPLOAD_DIR=./static/uploads
MAX_FILE_SIZE=52428800

# Security
ENCRYPTION_KEY=your_32_character_encryption_key_here
AUTH_TRUST_HOST=true

# ===== LLM/NLP INTEGRATION =====

# Local LLM Service (for development/Tauri)
LLM_SERVICE_URL=http://localhost:8000

# Production LLM APIs (for Vercel deployment)
GEMINI_API_KEY=your_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# Vector Database (Qdrant)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your_qdrant_api_key_if_needed

# Python NLP Service
PYTHON_NLP_SERVICE_URL=http://localhost:8001

# ======================
# MODEL CONFIGURATION
# ======================
# User-provided LLM model path (GGUF format only)
LLM_MODEL_PATH=
SENTENCE_MODEL=all-MiniLM-L6-v2
MODELS_DIR=./models
AI_MODELS_PATH=./ai_models

# ======================
# MULTIMODAL SETTINGS
# ======================
ENABLE_MULTIMODAL_ANALYSIS=true
MAX_VIDEO_DURATION=300
EVIDENCE_STORAGE_PATH=./evidence_storage

# ======================
# WHISPER SETTINGS
# ======================
WHISPER_MODEL_SIZE=base
WHISPER_LANGUAGE=auto
WHISPER_DEVICE=auto

# ======================
# YOLO SETTINGS
# ======================
YOLO_MODEL_SIZE=n
YOLO_CONFIDENCE_THRESHOLD=0.5
YOLO_DEVICE=auto

# ======================
# API CONFIGURATION
# ======================
API_HOST=127.0.0.1
API_PORT=8001
API_WORKERS=1
LOG_LEVEL=INFO

# ======================
# CACHE SETTINGS
# ======================
ENABLE_EMBEDDINGS_CACHE=true
CACHE_EXPIRY_HOURS=24
TEMP_DIR=./temp

# Cache Configuration
CACHE_TTL_DEFAULT=300000
CACHE_TTL_NLP=1800000
CACHE_TTL_RELATIONSHIPS=600000

# Feature Flags
ENABLE_AI_FEATURES=true
ENABLE_SEMANTIC_SEARCH=true
ENABLE_RAG=true
ENABLE_USER_FEEDBACK=true

# ⚠️  USER-PROVIDED MODELS ONLY ⚠️ 
# You must provide your own GGUF models for local LLM features
# The application will NOT download or provide any models automatically
LLM_MODEL_PATH=  # Set this to your GGUF model file path
MODELS_DIR=./user_models/  # Directory for user-uploaded models

# Performance Settings
MAX_CONCURRENT_LLM_REQUESTS=3
LLM_REQUEST_TIMEOUT=30000